{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89e6270-9fe8-4332-ba5a-e9e2d79cecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26712d72-8d3e-45f6-a35a-35fa45d85b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063260ee-ab01-4fe3-8db1-fa72025e6888",
   "metadata": {},
   "source": [
    "# Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c934e905-0a84-4cdd-9f45-bb5848bfd3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d73aa64-5f2d-4e65-862c-8c382af5e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8eb1f9-1f88-4316-9c74-86c90d076c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12845 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('./ASL_DATASET/dataSet/trainingData',                                \n",
    "                                                 target_size = (128, 128),\n",
    "                                                 batch_size = 10,\n",
    "                                                 color_mode = 'grayscale',                                \n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b0f27d-1c6b-4bb2-9570-66acca2ed0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4268 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('./ASL_DATASET/dataSet/testingData',\n",
    "                                            target_size = (128, 128),                                  \n",
    "                                            batch_size = 10,        \n",
    "                                            color_mode = 'grayscale',\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb8cce-0859-4232-9446-2828f4778c95",
   "metadata": {},
   "source": [
    "# Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb4598-5435-43a4-bbb0-7eb349fda2a9",
   "metadata": {},
   "source": [
    "## Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d38447b-ed48-426e-b1c8-4d0b4336d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 09:33:34.650236: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a748d1c-d31c-4b5b-b579-8249537450dd",
   "metadata": {},
   "source": [
    "#### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc9fcfa-6a86-4432-8880-88e9ab33dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(tf.keras.layers.Conv2D(filters=32,\n",
    "                                     kernel_size=3, \n",
    "                                     padding=\"same\", \n",
    "                                     activation=\"relu\", \n",
    "                                     input_shape=[128, 128, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74345ef0-f22f-46dc-8b7d-05993b843f45",
   "metadata": {},
   "source": [
    "#### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb539aff-451a-411d-ad78-335f399d701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(tf.keras.layers.MaxPool2D(pool_size=2, \n",
    "                                         strides=2, \n",
    "                                         padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807a966-dafa-4f04-ae54-74f8077fe45d",
   "metadata": {},
   "source": [
    "#### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad92827-e075-482d-b6b7-05198ab04e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(tf.keras.layers.Conv2D(filters=32, \n",
    "                                      kernel_size=3, \n",
    "                                      padding=\"same\", \n",
    "                                      activation=\"relu\"))\n",
    "\n",
    "classifier.add(tf.keras.layers.MaxPool2D(pool_size=2, \n",
    "                                         strides=2, \n",
    "                                         padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49318b8e-1d27-4bef-87f7-9a3326ca8ba0",
   "metadata": {},
   "source": [
    "#### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "748cb73e-6bfe-43bf-8581-aaae8e046328",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52791b28-7d10-4fc9-ad4e-5e49c33eaba2",
   "metadata": {},
   "source": [
    "#### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4e5112e-7444-46ed-a99e-d6e43f7f2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(tf.keras.layers.Dense(units=128, \n",
    "                                     activation='relu'))\n",
    "classifier.add(tf.keras.layers.Dropout(0.40))\n",
    "classifier.add(tf.keras.layers.Dense(units=96, activation='relu'))\n",
    "classifier.add(tf.keras.layers.Dropout(0.40))\n",
    "classifier.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "classifier.add(tf.keras.layers.Dense(units=27, activation='softmax')) # softmax for more than 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b013826-5e21-448a-b644-5efd476a0c7b",
   "metadata": {},
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81dedc-81dc-4fbc-b47f-9aa34daaee00",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656ff486-f1ae-48ab-86c9-1521149d9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', \n",
    "                   loss = 'categorical_crossentropy', \n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d11fc4-1bac-418c-9af5-5abf37fba9e6",
   "metadata": {},
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773cab54-dd43-456d-8946-6297140ea535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 64, 64, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 32, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               4194432   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 96)                12384     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 27)                1755      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,224,347\n",
      "Trainable params: 4,224,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5294eebd-d07a-4419-96ba-738123964aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1285/1285 [==============================] - 196s 152ms/step - loss: 0.3334 - accuracy: 0.8920 - val_loss: 0.0188 - val_accuracy: 0.9958\n",
      "Epoch 2/15\n",
      "1285/1285 [==============================] - 188s 146ms/step - loss: 0.3089 - accuracy: 0.8989 - val_loss: 0.0171 - val_accuracy: 0.9963\n",
      "Epoch 3/15\n",
      "1285/1285 [==============================] - 190s 147ms/step - loss: 0.2609 - accuracy: 0.9167 - val_loss: 0.0226 - val_accuracy: 0.9941\n",
      "Epoch 4/15\n",
      "1285/1285 [==============================] - 188s 146ms/step - loss: 0.2389 - accuracy: 0.9237 - val_loss: 0.0125 - val_accuracy: 0.9974\n",
      "Epoch 5/15\n",
      "1285/1285 [==============================] - 187s 145ms/step - loss: 0.2262 - accuracy: 0.9281 - val_loss: 0.0096 - val_accuracy: 0.9965\n",
      "Epoch 6/15\n",
      "1285/1285 [==============================] - 187s 146ms/step - loss: 0.2239 - accuracy: 0.9304 - val_loss: 0.0086 - val_accuracy: 0.9979\n",
      "Epoch 7/15\n",
      "1285/1285 [==============================] - 192s 149ms/step - loss: 0.1927 - accuracy: 0.9415 - val_loss: 0.0137 - val_accuracy: 0.9963\n",
      "Epoch 8/15\n",
      "1285/1285 [==============================] - 186s 145ms/step - loss: 0.1982 - accuracy: 0.9368 - val_loss: 0.0096 - val_accuracy: 0.9981\n",
      "Epoch 9/15\n",
      "1285/1285 [==============================] - 190s 148ms/step - loss: 0.1779 - accuracy: 0.9458 - val_loss: 0.0339 - val_accuracy: 0.9878\n",
      "Epoch 10/15\n",
      "1285/1285 [==============================] - 186s 145ms/step - loss: 0.1675 - accuracy: 0.9503 - val_loss: 0.0142 - val_accuracy: 0.9965\n",
      "Epoch 11/15\n",
      "1285/1285 [==============================] - 187s 145ms/step - loss: 0.1565 - accuracy: 0.9506 - val_loss: 0.0036 - val_accuracy: 0.9988\n",
      "Epoch 12/15\n",
      "1285/1285 [==============================] - 187s 146ms/step - loss: 0.1650 - accuracy: 0.9497 - val_loss: 0.0034 - val_accuracy: 0.9991\n",
      "Epoch 13/15\n",
      "1285/1285 [==============================] - 184s 143ms/step - loss: 0.1428 - accuracy: 0.9552 - val_loss: 0.0045 - val_accuracy: 0.9984\n",
      "Epoch 14/15\n",
      "1285/1285 [==============================] - 185s 144ms/step - loss: 0.1344 - accuracy: 0.9586 - val_loss: 0.0018 - val_accuracy: 0.9995\n",
      "Epoch 15/15\n",
      "1285/1285 [==============================] - 189s 147ms/step - loss: 0.1378 - accuracy: 0.9554 - val_loss: 0.0035 - val_accuracy: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd99ef47fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(training_set,\n",
    "                  epochs = 15,\n",
    "                  validation_data = test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01a4bf-a435-4ad5-b3fa-457102a739b8",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ae192f-1b88-4aca-80c0-fdc133ca8a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Weights saved\n"
     ]
    }
   ],
   "source": [
    "model_json = classifier.to_json()\n",
    "with open(\"model_new.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print('Model Saved')\n",
    "classifier.save_weights('model_new.h5')\n",
    "print('Weights saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "555a345f-eb72-4cf7-a236-9dd05396932c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/u184497/ASL'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b3cd1f-ac6f-4a1f-a4a0-e2e58564fc73",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;34m/usr/lib/python3.8/ast.py\u001b[0m, in \u001b[0;32mparse\u001b[0m:\nLine \u001b[0;34m47\u001b[0m:    \u001b[34mreturn\u001b[39;49;00m \u001b[36mcompile\u001b[39;49;00m(source, filename, mode, flags,\n",
      "\u001b[0;31mSyntaxError\u001b[0m: invalid syntax (<string>, line 1)\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ede68d-9e87-47eb-b6a8-d1d55c078590",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'ModuleNotFoundError'>",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "mphands = mp.solutions.hands\n",
    "hand = mphands.Hands(max_num_hands=2, min_detection_confidence=0.75)\n",
    "mpdraw = mp.solutions.drawing_utils\n",
    "st.title(\"Sign Language Classification\")\n",
    "st.header(\"Gesture Model\")\n",
    "all_classes = os.listdir(\"./ASL\")\n",
    "\n",
    "\n",
    "@st.cache(hash_funcs={'tensorflow.python.keras.utils.object_identity.ObjectIdentityDictionary': id}, allow_output_mutation=True)\n",
    "def model_upload():\n",
    "    model = load_model(\"./model_new.h5\")\n",
    "    print(\"Loading\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, image):\n",
    "    img = np.array(image)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "    img = img / 255\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img)\n",
    "    pred = np.argmax(prediction, axis=-1)\n",
    "    return pred, prediction[0][pred[0]]\n",
    "\n",
    "\n",
    "run = st.checkbox('Run')\n",
    "FRAME_WINDOW = st.image([])\n",
    "camera = cv2.VideoCapture(0)\n",
    "if run is False:\n",
    "    camera.release()\n",
    "\n",
    "while run:\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    _, frame = camera.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hand.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            for id, ld in enumerate(landmarks.landmark):\n",
    "                h, w, channels = frame.shape\n",
    "                x_points.append(int(ld.x * w))\n",
    "                y_points.append(int(ld.y * h))\n",
    "\n",
    "            a1 = (int(max(y_points) + 30), int(min(y_points) - 30))\n",
    "            a2 = (int(max(x_points) + 30), int(min(x_points) - 30))\n",
    "        cv2.rectangle(frame, (a2[1], a1[1]), (a2[0], a1[0]), (0, 255, 0), 3)\n",
    "        if len(x_points) == 21 or len(x_points) == 42:\n",
    "            target = frame[a1[1]:a1[0], a2[1]:a2[0]]\n",
    "\n",
    "            if len(target) > 0:\n",
    "                m = model_upload()\n",
    "                p, num = predict(m, frame)\n",
    "                cv2.putText(frame, str(all_classes[p[0]]) + \" \" + str(100*num), (80, 80), cv2.FONT_ITALIC, 2, (255, 100, 100), 2)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    FRAME_WINDOW.image(frame_rgb)\n",
    "\n",
    "\n",
    "else:\n",
    "    st.write('Stopped')\n",
    "    camera.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f99f2-a130-4e7d-9019-d73ec3a49555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython Raw)",
   "language": "python",
   "name": "xpython-raw"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
